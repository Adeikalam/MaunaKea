{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:/MaunaKea/TrainingSetImagesDir/\"\n",
    "patients = {}\n",
    "\n",
    "# Creer un dict avec tous les paths des images par patient\n",
    "for i in range(61):\n",
    "    patients[str(i)] = []\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\"_\" + str(i) + \".png\"):\n",
    "            patients[str(i)] += [file]\n",
    "\n",
    "# creer un json pour recup√©rer le dict apres\n",
    "with open('unsorted_patients.txt', 'w') as json_file:\n",
    "    json.dump(patients, json_file)\n",
    "\n",
    "targets = pd.read_csv(\"train_target.csv\")\n",
    "targets['patient'] = targets['image_filename'].map(lambda x: x.partition('_')[-1].partition('_')[-1].partition('.')[0])\n",
    "targets = targets.sort_values(by = ['patient', 'image_filename']).set_index(keys = ['image_filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(patients.keys())\n",
    "for key in keys:\n",
    "    if (patients[key] == []):\n",
    "        patients.pop(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = targets.loc[patients['10'] + patients['8'] + patients['7'] + patients['6']]# + patients['1'] + patients['0']]\n",
    "training_set = targets.drop(validation_set.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_training_path = \"D:/MaunaKea/TrainingSetImagesDir/\"\n",
    "small_training_images = np.zeros((training_set.shape[0], 224,224, 3))\n",
    "small_training_classes = np.zeros(training_set.shape[0])\n",
    "\n",
    "small_validation_path = \"D:/MaunaKea/TrainingSetImagesDir/\"\n",
    "small_validation_images = np.zeros((validation_set.shape[0], 224, 224, 3))\n",
    "small_validation_classes = np.zeros(validation_set.shape[0])\n",
    "\n",
    "i = 0\n",
    "for image_path in training_set.index:\n",
    "    small_training_images[i] = cv2.resize(cv2.imread(small_training_path + image_path), (224, 224))\n",
    "    #small_training_images[i] = preprocess_input(small_training_images[i])\n",
    "    small_training_classes[i] = training_set.iloc[i]['class_number']\n",
    "    i += 1\n",
    "    \n",
    "i = 0\n",
    "for image_path in validation_set.index:\n",
    "    small_validation_images[i] = cv2.resize(cv2.imread(small_validation_path + image_path), (224, 224))\n",
    "    small_validation_images[i] = preprocess_input(small_validation_images[i])\n",
    "    small_validation_classes[i] = validation_set.iloc[i]['class_number']\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_training_classes = to_categorical(small_training_classes)\n",
    "small_validation_classes = to_categorical(small_validation_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning Block 5   ~91% val acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"vgg16_augmented_nornal_images_no_denoised_85_val_acc.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_4 False\n",
      "1 block1_conv1 False\n",
      "2 block1_conv2 False\n",
      "3 block1_pool False\n",
      "4 block2_conv1 False\n",
      "5 block2_conv2 False\n",
      "6 block2_pool False\n",
      "7 block3_conv1 False\n",
      "8 block3_conv2 False\n",
      "9 block3_conv3 False\n",
      "10 block3_pool False\n",
      "11 block4_conv1 False\n",
      "12 block4_conv2 False\n",
      "13 block4_conv3 False\n",
      "14 block4_pool False\n",
      "15 block5_conv1 True\n",
      "16 block5_conv2 True\n",
      "17 block5_conv3 True\n",
      "18 block5_pool True\n",
      "19 flatten_4 True\n",
      "20 dense_10 True\n",
      "21 batch_normalization_7 True\n",
      "22 activation_7 True\n",
      "23 dropout_7 True\n",
      "24 dense_11 True\n",
      "25 batch_normalization_8 True\n",
      "26 activation_8 True\n",
      "27 dropout_8 True\n",
      "28 dense_12 True\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers[:15]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[15:]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "for i, layer in enumerate(model.layers):\n",
    "    print(i, layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rotation_range = 180,\n",
    "   #                          brightness_range = [0.8, 1.2],\n",
    "                             width_shift_range = 30,\n",
    "                             height_shift_range = 30,\n",
    "                             zoom_range = [1.0, 1.2],\n",
    "                             preprocessing_function = preprocess_input,\n",
    "                             horizontal_flip = True,\n",
    "                             vertical_flip = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "133/132 [==============================] - 55s 411ms/step - loss: 0.2862 - acc: 0.8954 - val_loss: 0.4254 - val_acc: 0.8523\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.85231, saving model to C:\\Users\\Pierre\\saved_models\\finetuned_VGG16_model.001.h5\n",
      "Epoch 2/15\n",
      "133/132 [==============================] - 50s 379ms/step - loss: 0.2301 - acc: 0.9166 - val_loss: 0.3882 - val_acc: 0.8656\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.85231 to 0.86564, saving model to C:\\Users\\Pierre\\saved_models\\finetuned_VGG16_model.002.h5\n",
      "Epoch 3/15\n",
      "133/132 [==============================] - 49s 367ms/step - loss: 0.2208 - acc: 0.9236 - val_loss: 0.5076 - val_acc: 0.8174\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.86564\n",
      "Epoch 4/15\n",
      "133/132 [==============================] - 49s 366ms/step - loss: 0.1830 - acc: 0.9301 - val_loss: 0.4737 - val_acc: 0.8072\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.86564\n",
      "Epoch 5/15\n",
      "133/132 [==============================] - 48s 364ms/step - loss: 0.1745 - acc: 0.9363 - val_loss: 0.2612 - val_acc: 0.8964\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.86564 to 0.89641, saving model to C:\\Users\\Pierre\\saved_models\\finetuned_VGG16_model.005.h5\n",
      "Epoch 6/15\n",
      "133/132 [==============================] - 49s 366ms/step - loss: 0.1653 - acc: 0.9379 - val_loss: 0.3371 - val_acc: 0.8749\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.89641\n",
      "Epoch 7/15\n",
      "133/132 [==============================] - 50s 373ms/step - loss: 0.1567 - acc: 0.9445 - val_loss: 0.3694 - val_acc: 0.8646\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.89641\n",
      "Epoch 8/15\n",
      "133/132 [==============================] - 49s 371ms/step - loss: 0.1500 - acc: 0.9496 - val_loss: 0.2563 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.89641 to 0.89846, saving model to C:\\Users\\Pierre\\saved_models\\finetuned_VGG16_model.008.h5\n",
      "Epoch 9/15\n",
      "133/132 [==============================] - 49s 367ms/step - loss: 0.1335 - acc: 0.9551 - val_loss: 0.2617 - val_acc: 0.9087\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.89846 to 0.90872, saving model to C:\\Users\\Pierre\\saved_models\\finetuned_VGG16_model.009.h5\n",
      "Epoch 10/15\n",
      "133/132 [==============================] - 49s 367ms/step - loss: 0.1336 - acc: 0.9533 - val_loss: 0.3584 - val_acc: 0.8718\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.90872\n",
      "Epoch 11/15\n",
      "133/132 [==============================] - 49s 368ms/step - loss: 0.1497 - acc: 0.9467 - val_loss: 0.3496 - val_acc: 0.8779\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.90872\n",
      "Epoch 12/15\n",
      "133/132 [==============================] - 49s 368ms/step - loss: 0.1281 - acc: 0.9561 - val_loss: 0.2483 - val_acc: 0.9015\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.90872\n",
      "Epoch 13/15\n",
      "133/132 [==============================] - 49s 371ms/step - loss: 0.1175 - acc: 0.9589 - val_loss: 0.3688 - val_acc: 0.8646\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.90872\n",
      "Epoch 14/15\n",
      "133/132 [==============================] - 49s 368ms/step - loss: 0.1203 - acc: 0.9568 - val_loss: 0.9160 - val_acc: 0.7569\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.90872\n",
      "Epoch 15/15\n",
      "133/132 [==============================] - 49s 365ms/step - loss: 0.0993 - acc: 0.9662 - val_loss: 0.2391 - val_acc: 0.9067\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.90872\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-5\n",
    "batch_size = 32\n",
    "n_epochs = 15\n",
    "\n",
    "optimizer = Adam(lr = lr)\n",
    "lr_reducer = ReduceLROnPlateau(factor=0.1,\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-7)\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_type = 'VGG16'\n",
    "model_name = 'finetuned_%s_model.{epoch:03d}.h5' % model_type\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "filepath = os.path.join(save_dir, model_name)\n",
    "\n",
    "# Prepare callbacks for model saving and for learning rate adjustment.\n",
    "checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                             monitor='val_acc',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True)\n",
    "callbacks = [lr_reducer, checkpoint]\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
    "history = model.fit_generator(datagen.flow(small_training_images, small_training_classes, batch_size),\n",
    "                                  steps_per_epoch = small_training_images.shape[0]/(batch_size * 2), epochs = n_epochs,\n",
    "                                  validation_data = (small_validation_images, small_validation_classes),\n",
    "                                  callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning Block 4 ~93% val acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"C:/Users/Pierre/saved_models/finetuned_VGG16_model.009.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_4 False\n",
      "1 block1_conv1 False\n",
      "2 block1_conv2 False\n",
      "3 block1_pool False\n",
      "4 block2_conv1 False\n",
      "5 block2_conv2 False\n",
      "6 block2_pool False\n",
      "7 block3_conv1 False\n",
      "8 block3_conv2 False\n",
      "9 block3_conv3 False\n",
      "10 block3_pool False\n",
      "11 block4_conv1 True\n",
      "12 block4_conv2 True\n",
      "13 block4_conv3 True\n",
      "14 block4_pool True\n",
      "15 block5_conv1 True\n",
      "16 block5_conv2 True\n",
      "17 block5_conv3 True\n",
      "18 block5_pool True\n",
      "19 flatten_4 True\n",
      "20 dense_10 True\n",
      "21 batch_normalization_7 True\n",
      "22 activation_7 True\n",
      "23 dropout_7 True\n",
      "24 dense_11 True\n",
      "25 batch_normalization_8 True\n",
      "26 activation_8 True\n",
      "27 dropout_8 True\n",
      "28 dense_12 True\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers[:11]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[11:]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "for i, layer in enumerate(model.layers):\n",
    "    print(i, layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rotation_range = 180,\n",
    "   #                          brightness_range = [0.8, 1.2],\n",
    "                             width_shift_range = 30,\n",
    "                             height_shift_range = 30,\n",
    "                             zoom_range = [1.0, 1.2],\n",
    "                             preprocessing_function = preprocess_input,\n",
    "                             horizontal_flip = True,\n",
    "                             vertical_flip = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "133/132 [==============================] - 52s 390ms/step - loss: 0.1448 - acc: 0.9445 - val_loss: 0.2570 - val_acc: 0.9108\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.91077, saving model to C:\\Users\\Pierre\\saved_models\\finetuned_block4_VGG16_model.001.h5\n",
      "Epoch 2/30\n",
      "133/132 [==============================] - 48s 360ms/step - loss: 0.1274 - acc: 0.9554 - val_loss: 0.3154 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.91077\n",
      "Epoch 3/30\n",
      "133/132 [==============================] - 48s 358ms/step - loss: 0.1283 - acc: 0.9523 - val_loss: 0.2339 - val_acc: 0.9128\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.91077 to 0.91282, saving model to C:\\Users\\Pierre\\saved_models\\finetuned_block4_VGG16_model.003.h5\n",
      "Epoch 4/30\n",
      "133/132 [==============================] - 47s 357ms/step - loss: 0.1191 - acc: 0.9540 - val_loss: 0.2329 - val_acc: 0.9118\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.91282\n",
      "Epoch 5/30\n",
      "133/132 [==============================] - 48s 357ms/step - loss: 0.1174 - acc: 0.9601 - val_loss: 0.2619 - val_acc: 0.9015\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.91282\n",
      "Epoch 6/30\n",
      "133/132 [==============================] - 47s 357ms/step - loss: 0.1151 - acc: 0.9581 - val_loss: 0.2419 - val_acc: 0.9046\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.91282\n",
      "Epoch 7/30\n",
      "133/132 [==============================] - 48s 358ms/step - loss: 0.1134 - acc: 0.9586 - val_loss: 0.1983 - val_acc: 0.9262\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.91282 to 0.92615, saving model to C:\\Users\\Pierre\\saved_models\\finetuned_block4_VGG16_model.007.h5\n",
      "Epoch 8/30\n",
      "133/132 [==============================] - 48s 362ms/step - loss: 0.1291 - acc: 0.9575 - val_loss: 0.2141 - val_acc: 0.9241\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.92615\n",
      "Epoch 9/30\n",
      "133/132 [==============================] - 48s 358ms/step - loss: 0.1235 - acc: 0.9572 - val_loss: 0.2086 - val_acc: 0.9210\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.92615\n",
      "Epoch 10/30\n",
      "133/132 [==============================] - 48s 358ms/step - loss: 0.1071 - acc: 0.9613 - val_loss: 0.2516 - val_acc: 0.9108\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.92615\n",
      "Epoch 11/30\n",
      "133/132 [==============================] - 48s 358ms/step - loss: 0.1072 - acc: 0.9652 - val_loss: 0.2038 - val_acc: 0.9251\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.92615\n",
      "Epoch 12/30\n",
      "133/132 [==============================] - 48s 359ms/step - loss: 0.1117 - acc: 0.9631 - val_loss: 0.2273 - val_acc: 0.9118\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.92615\n",
      "Epoch 13/30\n",
      "133/132 [==============================] - 48s 358ms/step - loss: 0.1052 - acc: 0.9641 - val_loss: 0.2525 - val_acc: 0.9077\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.92615\n",
      "Epoch 14/30\n",
      "133/132 [==============================] - 48s 358ms/step - loss: 0.0991 - acc: 0.9643 - val_loss: 0.2580 - val_acc: 0.9077\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.92615\n",
      "Epoch 15/30\n",
      "133/132 [==============================] - 48s 358ms/step - loss: 0.1004 - acc: 0.9662 - val_loss: 0.2512 - val_acc: 0.9077\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.92615\n",
      "Epoch 16/30\n",
      "133/132 [==============================] - 47s 354ms/step - loss: 0.1084 - acc: 0.9596 - val_loss: 0.2562 - val_acc: 0.9067\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.92615\n",
      "Epoch 17/30\n",
      "133/132 [==============================] - 48s 362ms/step - loss: 0.1062 - acc: 0.9605 - val_loss: 0.2576 - val_acc: 0.9056\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.92615\n",
      "Epoch 18/30\n",
      "133/132 [==============================] - 48s 359ms/step - loss: 0.1097 - acc: 0.9634 - val_loss: 0.2635 - val_acc: 0.9036\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.92615\n",
      "Epoch 19/30\n",
      "133/132 [==============================] - 48s 357ms/step - loss: 0.1023 - acc: 0.9626 - val_loss: 0.2480 - val_acc: 0.9118\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.92615\n",
      "Epoch 20/30\n",
      "133/132 [==============================] - 48s 357ms/step - loss: 0.1028 - acc: 0.9639 - val_loss: 0.2415 - val_acc: 0.9179\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.92615\n",
      "Epoch 21/30\n",
      "133/132 [==============================] - 48s 362ms/step - loss: 0.0912 - acc: 0.9650 - val_loss: 0.2538 - val_acc: 0.9128\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.92615\n",
      "Epoch 22/30\n",
      "133/132 [==============================] - 48s 361ms/step - loss: 0.0966 - acc: 0.9655 - val_loss: 0.2603 - val_acc: 0.9056\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.92615\n",
      "Epoch 23/30\n",
      "133/132 [==============================] - 48s 360ms/step - loss: 0.0899 - acc: 0.9662 - val_loss: 0.2552 - val_acc: 0.9128\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.92615\n",
      "Epoch 24/30\n",
      "133/132 [==============================] - 47s 357ms/step - loss: 0.1071 - acc: 0.9628 - val_loss: 0.2576 - val_acc: 0.9118\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.92615\n",
      "Epoch 25/30\n",
      "133/132 [==============================] - 47s 357ms/step - loss: 0.1018 - acc: 0.9650 - val_loss: 0.2559 - val_acc: 0.9108\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.92615\n",
      "Epoch 26/30\n",
      "133/132 [==============================] - 47s 357ms/step - loss: 0.1001 - acc: 0.9648 - val_loss: 0.2696 - val_acc: 0.9005\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.92615\n",
      "Epoch 27/30\n",
      "133/132 [==============================] - 47s 355ms/step - loss: 0.1116 - acc: 0.9596 - val_loss: 0.2476 - val_acc: 0.9128\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.92615\n",
      "Epoch 28/30\n",
      "133/132 [==============================] - 47s 356ms/step - loss: 0.1126 - acc: 0.9597 - val_loss: 0.2647 - val_acc: 0.9036\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.92615\n",
      "Epoch 29/30\n",
      "133/132 [==============================] - 47s 356ms/step - loss: 0.1053 - acc: 0.9629 - val_loss: 0.2697 - val_acc: 0.9026\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.92615\n",
      "Epoch 30/30\n",
      "133/132 [==============================] - 47s 356ms/step - loss: 0.0947 - acc: 0.9650 - val_loss: 0.2429 - val_acc: 0.9149\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.92615\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-6\n",
    "batch_size = 32\n",
    "n_epochs = 30\n",
    "\n",
    "optimizer = Adam(lr = lr)\n",
    "lr_reducer = ReduceLROnPlateau(factor=0.1,\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-7)\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_type = 'VGG16'\n",
    "model_name = 'finetuned_block4_%s_model.{epoch:03d}.h5' % model_type\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "filepath = os.path.join(save_dir, model_name)\n",
    "\n",
    "# Prepare callbacks for model saving and for learning rate adjustment.\n",
    "checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                             monitor='val_acc',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True)\n",
    "callbacks = [lr_reducer, checkpoint]\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
    "history = model.fit_generator(datagen.flow(small_training_images, small_training_classes, batch_size),\n",
    "                                  steps_per_epoch = small_training_images.shape[0]/(batch_size * 2), epochs = n_epochs,\n",
    "                                  validation_data = (small_validation_images, small_validation_classes),\n",
    "                                  callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning Block 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"C:/Users/Pierre/saved_models/finetuned_block4_VGG16_model.007.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_4 False\n",
      "1 block1_conv1 False\n",
      "2 block1_conv2 False\n",
      "3 block1_pool False\n",
      "4 block2_conv1 False\n",
      "5 block2_conv2 False\n",
      "6 block2_pool False\n",
      "7 block3_conv1 True\n",
      "8 block3_conv2 True\n",
      "9 block3_conv3 True\n",
      "10 block3_pool True\n",
      "11 block4_conv1 True\n",
      "12 block4_conv2 True\n",
      "13 block4_conv3 True\n",
      "14 block4_pool True\n",
      "15 block5_conv1 True\n",
      "16 block5_conv2 True\n",
      "17 block5_conv3 True\n",
      "18 block5_pool True\n",
      "19 flatten_4 True\n",
      "20 dense_10 True\n",
      "21 batch_normalization_7 True\n",
      "22 activation_7 True\n",
      "23 dropout_7 True\n",
      "24 dense_11 True\n",
      "25 batch_normalization_8 True\n",
      "26 activation_8 True\n",
      "27 dropout_8 True\n",
      "28 dense_12 True\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers[:7]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[7:]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "for i, layer in enumerate(model.layers):\n",
    "    print(i, layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rotation_range = 180,\n",
    "   #                          brightness_range = [0.8, 1.2],\n",
    "                             width_shift_range = 30,\n",
    "                             height_shift_range = 30,\n",
    "                             zoom_range = [1.0, 1.2],\n",
    "                             preprocessing_function = preprocess_input,\n",
    "                             horizontal_flip = True,\n",
    "                             vertical_flip = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "133/132 [==============================] - 61s 461ms/step - loss: 0.1059 - acc: 0.9596 - val_loss: 0.2705 - val_acc: 0.9046\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.90462, saving model to C:\\Users\\Pierre\\saved_models\\finetuned_block3_VGG16_model.001.h5\n",
      "Epoch 2/30\n",
      "133/132 [==============================] - 57s 431ms/step - loss: 0.1154 - acc: 0.9618 - val_loss: 0.2769 - val_acc: 0.9005\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.90462\n",
      "Epoch 3/30\n",
      "133/132 [==============================] - 57s 431ms/step - loss: 0.1135 - acc: 0.9596 - val_loss: 0.2751 - val_acc: 0.9046\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.90462\n",
      "Epoch 4/30\n",
      "133/132 [==============================] - 57s 431ms/step - loss: 0.1134 - acc: 0.9578 - val_loss: 0.2722 - val_acc: 0.9026\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.90462\n",
      "Epoch 5/30\n",
      "133/132 [==============================] - 57s 432ms/step - loss: 0.1008 - acc: 0.9617 - val_loss: 0.2532 - val_acc: 0.9046\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.90462\n",
      "Epoch 6/30\n",
      "133/132 [==============================] - 57s 431ms/step - loss: 0.1185 - acc: 0.9575 - val_loss: 0.2462 - val_acc: 0.9128\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.90462 to 0.91282, saving model to C:\\Users\\Pierre\\saved_models\\finetuned_block3_VGG16_model.006.h5\n",
      "Epoch 7/30\n",
      "133/132 [==============================] - 57s 431ms/step - loss: 0.1171 - acc: 0.9598 - val_loss: 0.2607 - val_acc: 0.9108\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.91282\n",
      "Epoch 8/30\n",
      "133/132 [==============================] - 57s 431ms/step - loss: 0.1124 - acc: 0.9576 - val_loss: 0.2544 - val_acc: 0.9097\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.91282\n",
      "Epoch 9/30\n",
      "133/132 [==============================] - 57s 431ms/step - loss: 0.1263 - acc: 0.9575 - val_loss: 0.2740 - val_acc: 0.9067\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.91282\n",
      "Epoch 10/30\n",
      "133/132 [==============================] - 57s 431ms/step - loss: 0.1109 - acc: 0.9596 - val_loss: 0.2795 - val_acc: 0.9015\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.91282\n",
      "Epoch 11/30\n",
      "133/132 [==============================] - 57s 432ms/step - loss: 0.1135 - acc: 0.9594 - val_loss: 0.2800 - val_acc: 0.9005\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.91282\n",
      "Epoch 12/30\n",
      "133/132 [==============================] - 57s 431ms/step - loss: 0.1015 - acc: 0.9605 - val_loss: 0.2688 - val_acc: 0.9026\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.91282\n",
      "Epoch 13/30\n",
      "133/132 [==============================] - 57s 432ms/step - loss: 0.1173 - acc: 0.9598 - val_loss: 0.2758 - val_acc: 0.9005\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.91282\n",
      "Epoch 14/30\n",
      "133/132 [==============================] - 57s 431ms/step - loss: 0.1097 - acc: 0.9628 - val_loss: 0.2594 - val_acc: 0.9056\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.91282\n",
      "Epoch 15/30\n",
      "133/132 [==============================] - 57s 431ms/step - loss: 0.1035 - acc: 0.9617 - val_loss: 0.2548 - val_acc: 0.9067\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.91282\n",
      "Epoch 16/30\n",
      "133/132 [==============================] - 57s 432ms/step - loss: 0.1163 - acc: 0.9591 - val_loss: 0.2559 - val_acc: 0.9067\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.91282\n",
      "Epoch 17/30\n",
      "133/132 [==============================] - 57s 431ms/step - loss: 0.1060 - acc: 0.9641 - val_loss: 0.2607 - val_acc: 0.9067\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.91282\n",
      "Epoch 18/30\n",
      "133/132 [==============================] - 57s 431ms/step - loss: 0.1096 - acc: 0.9605 - val_loss: 0.2675 - val_acc: 0.9036\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.91282\n",
      "Epoch 19/30\n",
      "133/132 [==============================] - 57s 431ms/step - loss: 0.1037 - acc: 0.9633 - val_loss: 0.2839 - val_acc: 0.9005\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.91282\n",
      "Epoch 20/30\n",
      "133/132 [==============================] - 57s 431ms/step - loss: 0.1200 - acc: 0.9603 - val_loss: 0.2726 - val_acc: 0.9067\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.91282\n",
      "Epoch 21/30\n",
      "133/132 [==============================] - 57s 431ms/step - loss: 0.1085 - acc: 0.9619 - val_loss: 0.2813 - val_acc: 0.9036\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.91282\n",
      "Epoch 22/30\n",
      "133/132 [==============================] - 57s 431ms/step - loss: 0.1064 - acc: 0.9608 - val_loss: 0.2643 - val_acc: 0.9056\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.91282\n",
      "Epoch 23/30\n",
      "133/132 [==============================] - 57s 431ms/step - loss: 0.1053 - acc: 0.9598 - val_loss: 0.2694 - val_acc: 0.9036\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.91282\n",
      "Epoch 24/30\n",
      "133/132 [==============================] - 57s 432ms/step - loss: 0.1127 - acc: 0.9588 - val_loss: 0.2805 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.91282\n",
      "Epoch 25/30\n",
      "133/132 [==============================] - 57s 432ms/step - loss: 0.0961 - acc: 0.9666 - val_loss: 0.2676 - val_acc: 0.9046\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.91282\n",
      "Epoch 26/30\n",
      "133/132 [==============================] - 57s 431ms/step - loss: 0.1125 - acc: 0.9638 - val_loss: 0.2697 - val_acc: 0.9046\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.91282\n",
      "Epoch 27/30\n",
      "133/132 [==============================] - 57s 432ms/step - loss: 0.1147 - acc: 0.9610 - val_loss: 0.2886 - val_acc: 0.8974\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.91282\n",
      "Epoch 28/30\n",
      "133/132 [==============================] - 57s 431ms/step - loss: 0.0978 - acc: 0.9643 - val_loss: 0.2660 - val_acc: 0.9046\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.91282\n",
      "Epoch 29/30\n",
      "133/132 [==============================] - 57s 431ms/step - loss: 0.1143 - acc: 0.9598 - val_loss: 0.2771 - val_acc: 0.9015\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.91282\n",
      "Epoch 30/30\n",
      "133/132 [==============================] - 57s 431ms/step - loss: 0.1066 - acc: 0.9618 - val_loss: 0.2755 - val_acc: 0.9026\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.91282\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-7\n",
    "batch_size = 32\n",
    "n_epochs = 30\n",
    "\n",
    "optimizer = Adam(lr = lr)\n",
    "lr_reducer = ReduceLROnPlateau(factor=0.1,\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-7)\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_type = 'VGG16'\n",
    "model_name = 'finetuned_block3_%s_model.{epoch:03d}.h5' % model_type\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "filepath = os.path.join(save_dir, model_name)\n",
    "\n",
    "# Prepare callbacks for model saving and for learning rate adjustment.\n",
    "checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                             monitor='val_acc',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True)\n",
    "callbacks = [lr_reducer, checkpoint]\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
    "history = model.fit_generator(datagen.flow(small_training_images, small_training_classes, batch_size),\n",
    "                                  steps_per_epoch = small_training_images.shape[0]/(batch_size * 2), epochs = n_epochs,\n",
    "                                  validation_data = (small_validation_images, small_validation_classes),\n",
    "                                  callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"C:/Users/Pierre/saved_models/finetuned_block4_VGG16_model.007.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_train = preprocess_input(small_training_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_pred = model.predict(preprocessed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9671821508676661"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(small_training_classes.argmax(axis = 1), train_pred.argmax(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = model.predict(small_validation_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9261538461538461"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(small_validation_classes.argmax(axis = 1), val_pred.argmax(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 813,    3,    2,    7],\n",
       "       [   8, 2956,   16,   22],\n",
       "       [  19,   18, 1081,   28],\n",
       "       [  53,   65,   37, 3343]], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(small_training_classes.argmax(axis = 1), train_pred.argmax(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[602,   0,   5,  37],\n",
       "       [  1, 149,  23,   2],\n",
       "       [  0,   0,  58,   2],\n",
       "       [  0,   2,   0,  94]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(small_validation_classes.argmax(axis = 1), val_pred.argmax(axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image per Image Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"D:/MaunaKea/submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_path = \"D:/MaunaKea/TestSetImagesDir/\"\n",
    "test_images = np.zeros((submission['image_name'].shape[0], 224, 224, 3))\n",
    "\n",
    "\n",
    "for i,image_path in enumerate(submission['image_name']):\n",
    "    image = cv2.imread(test_path + image_path)\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    image = image.reshape(1, 224, 224, 3)\n",
    "    test_images[i] = image\n",
    "\n",
    "test_images = preprocess_input(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['predictions'] = model.predict(test_images).argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    577\n",
       "1    453\n",
       "2    366\n",
       "0    319\n",
       "Name: predictions, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['predictions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('D:/MaunaKea/imperim_finetuned_vgg.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patient wise submition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = {}\n",
    "\n",
    "# Creer un dict avec tous les paths des images par patient\n",
    "for i in range(61):\n",
    "    patients[str(i)] = []\n",
    "    for file in os.listdir(test_path):\n",
    "        if file.endswith(\"_\" + str(i) + \".png\"):\n",
    "            patients[str(i)] += [file]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(patients.keys())\n",
    "for key in keys:\n",
    "    if (patients[key] == []):\n",
    "        patients.pop(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['9', '16', '20', '21', '26', '27', '28', '33', '37', '38', '39', '52', '56', '57', '58', '59', '60'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patients.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "16\n",
      "20\n",
      "21\n",
      "26\n",
      "27\n",
      "28\n",
      "33\n",
      "37\n",
      "38\n",
      "39\n",
      "52\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "patients_pred = {}\n",
    "for patient in patients.keys():\n",
    "    patient_images = np.zeros((len(patients[patient]), 224, 224, 3))\n",
    "    for i,image_path in enumerate(patients[patient]):\n",
    "        image = cv2.imread(test_path + image_path)\n",
    "        image = cv2.resize(image, (224, 224))\n",
    "        image = preprocess_input(image).reshape(1, 224, 224, 3)\n",
    "        patient_images[i] = image\n",
    "    patients_pred[patient] = model.predict(patient_images).argmax(axis = 1)\n",
    "    print(patient)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_class = {}\n",
    "for patient in patients_pred.keys():\n",
    "    predicted_class = np.bincount(patients_pred[patient]).argmax()\n",
    "    for image_path in patients[patient]:\n",
    "        submission.loc[submission['image_name'] == image_path, 'predictions'] = predicted_class\n",
    "    patient_class[patient] = predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('D:/MaunaKea/finetuned_vgg_patient_wise.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'9': 0,\n",
       " '16': 0,\n",
       " '20': 3,\n",
       " '21': 2,\n",
       " '26': 3,\n",
       " '27': 1,\n",
       " '28': 3,\n",
       " '33': 0,\n",
       " '37': 1,\n",
       " '38': 3,\n",
       " '39': 2,\n",
       " '52': 2,\n",
       " '56': 1,\n",
       " '57': 0,\n",
       " '58': 2,\n",
       " '59': 3,\n",
       " '60': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
